# === Core model ===
model = "gpt-5"                 # v0.15.0+ defaults to gpt-5, set explicitly here
model_provider = "openai"

# === Autonomy & sandbox ===
approval_policy = "on-request"  # model asks when it needs to escalate; less friction than 'untrusted'
sandbox_mode    = "danger-full-access"  # write and read anywhere

# === Upgrade reasoning ===
# CLI has --reasoning {low,medium,high}; set top-level so it isn't dropped by profiles.
model_reasoning_effort  = "high"
model_reasoning_summary = "detailed"

# === Allow network inside the sandbox ===
[sandbox_workspace_write]
network_access = true  # enables HTTP(S) egress from commands the agent runs in the sandbox

# === Optional: convenience profiles you can opt into ===
[profiles.full_auto]                 # non-interactive automation while keeping write-guardrails
approval_policy = "never"
sandbox_mode    = "workspace-write"

[profiles.no_sandbox]                # truly removes the sandbox (writes anywhere) -- use with care
approval_policy = "never"
sandbox_mode    = "danger-full-access"


# === MCP servers ===
[mcp_servers.filesystem]
command = "npx"
args = ["-y", "mcp-server-filesystem", "/home/ubuntu/work/modular"]
env = {}

[mcp_servers.git]
command = "pipx"
args = ["run", "mcp-server-git", "-r", "/home/ubuntu/work/modular"]
env = {}

[mcp_servers.fetch]
command = "pipx"
args = ["run", "mcp-server-fetch"]
env = {}

[mcp_servers.time]
command = "pipx"
args = ["run", "mcp-server-time"]
env = {}

# Sequential Thinking server (PyPI: sequential-thinking-mcp)
[mcp_servers.sequential_thinking]
command = "pipx"
args = ["run", "sequential-thinking-mcp"]

[mcp_servers.linear]
command = "npx"
args = ["-y", "mcp-remote", "https://mcp.linear.app/sse"]
